# pipelines/6b_upsert_embeddings_local.py

"""
UPSERT EMBEDDINGS (LOCAL)

This script facilitates the process of populating a local Qdrant vector database
with pre-computed embeddings and their associated metadata. It's optimized for
local machine execution by leveraging on-disk storage for Qdrant collections,
which is crucial for handling large datasets without consuming excessive RAM.

The script's primary responsibilities are:
1. Locating the embeddings (.npy) and metadata payloads (.jsonl) generated by the
   upstream '6a_generate_embeddings_colab.py' script.
2. Establishing a connection to a running local Qdrant instance.
3. Recreating the target Qdrant collection with performance-oriented settings
   (on-disk vectors and HNSW graph).
4. Iterating through the data parts, upserting them into the collection in
   efficient, memory-friendly batches.

Prerequisites:
- A running local Qdrant instance (e.g., via Docker).
- The output files from '6a_generate_embeddings_colab.py' must be downloaded
  to the 'data/processed/embeddings/' directory.

Usage:
  python pipelines/6b_upsert_embeddings_local.py --market sp500 --collection-name financial_sp500_on_disk
"""

import os
import sys
import json
import time
import argparse
import glob
import uuid
import numpy as np

# A soft dependency check to provide a helpful error message if qdrant-client is not installed.
try:
    from qdrant_client import QdrantClient, models
    LIBRARIES_AVAILABLE = True
except ImportError as e:
    print(f"Required library not installed: {e}. Please run 'pip install qdrant-client numpy'")
    LIBRARIES_AVAILABLE = False

# --- Configuration ---
# The URL for the local Qdrant instance.
QDRANT_URL_LOCAL = "http://localhost:6333"
# The number of points (embedding + payload) to send to Qdrant in a single API call.
# This value is a trade-off between network overhead and memory usage.
QDRANT_UPSERT_BATCH_SIZE = 512
# The dimensionality of the vectors, determined by the embedding model used.
# 'BAAI/bge-base-en-v1.5' produces 768-dimensional vectors.
VECTOR_SIZE = 768
# An increased timeout for the Qdrant client to prevent interruptions during
# potentially long-running upsert or collection creation operations.
CLIENT_TIMEOUT_S = 60

def recreate_collection_on_disk(client: QdrantClient, collection_name: str) -> None:
    """
    Deletes and recreates a Qdrant collection with on-disk storage optimizations.

    This configuration is ideal for local development on machines with limited RAM,
    as it stores the vectors and the HNSW index graph on disk (memory-mapped)
    rather than holding them entirely in memory.

    @param client: An initialized QdrantClient instance.
    @param collection_name: The name of the collection to recreate.
    """
    print(f"Recreating collection '{collection_name}' in on-disk mode…")
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=models.VectorParams(
            size=VECTOR_SIZE,
            distance=models.Distance.COSINE,
            # Enables memory-mapping vectors from disk.
            on_disk=True
        ),
        # Enables memory-mapping the HNSW graph from disk.
        hnsw_config=models.HnswConfigDiff(on_disk=True)
    )
    print("  ✓ Collection ready.")


def main():
    """
    Main execution function to load pre-computed embeddings and payloads from
    local files and upsert them into a Qdrant collection.
    """
    if not LIBRARIES_AVAILABLE:
        sys.exit(1)

    # Set up command-line arguments for specifying the dataset and target collection.
    parser = argparse.ArgumentParser(description="Upsert pre-computed embeddings to a local Qdrant instance with on-disk storage.")
    parser.add_argument(
        "--market",
        type=str,
        required=True,
        help="The market index suffix for the input files (e.g., 'sp500')."
    )
    parser.add_argument(
        "--collection-name",
        type=str,
        required=True,
        help="The name of the Qdrant collection to create or use."
    )
    args = parser.parse_args()

    # --- Locate embedding/payload parts ---
    embeddings_dir = "data/processed/embeddings"
    saved_file_basename = f"embeddings_{args.market}"
    
    # Use glob to find all parts for the specified market. Sorting ensures
    # that corresponding embedding and payload files are aligned correctly.
    emb_files = sorted(glob.glob(os.path.join(embeddings_dir, f"{saved_file_basename}_embeddings_part_*.npy")))
    pay_files = sorted(glob.glob(os.path.join(embeddings_dir, f"{saved_file_basename}_payloads_part_*.jsonl")))

    if not emb_files or not pay_files:
        print(f"Error: No embedding/payload files found in '{embeddings_dir}' for market '{args.market}'.")
        print("Please ensure you have downloaded the files from Google Drive and they match the expected pattern.")
        return
    if len(emb_files) != len(pay_files):
        print(f"Error: Embedding/payload part count mismatch ({len(emb_files)} vs {len(pay_files)}).")
        return
    print(f"Found {len(emb_files)} part(s) to upsert for market '{args.market}'.")

    # --- Connect and Prepare collection ---
    print(f"\nConnecting to Qdrant at {QDRANT_URL_LOCAL} …")
    client = QdrantClient(url=QDRANT_URL_LOCAL, timeout=CLIENT_TIMEOUT_S)
    print("  ✓ Connected. Existing collections:", [c.name for c in client.get_collections().collections])
    
    recreate_collection_on_disk(client, args.collection_name)

    # --- Upsert loop ---
    total_points = 0
    # A buffer to hold points before sending them in a batch.
    buf: list[models.PointStruct] = []
    t0 = time.time()

    # Process each pair of embedding and payload files.
    for part_idx, (emb_fp, pay_fp) in enumerate(zip(emb_files, pay_files), start=1):
        print(f"\nPart {part_idx}/{len(emb_files)}  →  {os.path.basename(emb_fp)} & {os.path.basename(pay_fp)}")

        vectors = np.load(emb_fp).tolist()
        payloads = [json.loads(line) for line in open(pay_fp, encoding="utf-8") if line.strip()]

        if len(vectors) != len(payloads):
            print("  ⚠ Skipping – vector/payload length mismatch.")
            continue

        # Create PointStruct objects and add them to the buffer.
        for vec, pld in zip(vectors, payloads):
            # Using UUIDs for point IDs is a robust way to ensure uniqueness.
            buf.append(models.PointStruct(id=str(uuid.uuid4()), vector=vec, payload=pld))
            
            # When the buffer is full, upsert the batch to Qdrant.
            if len(buf) >= QDRANT_UPSERT_BATCH_SIZE:
                # `wait=True` makes the call synchronous, ensuring the operation is
                # complete before the script proceeds. This is useful for batch scripts.
                client.upsert(args.collection_name, buf, wait=True)
                total_points += len(buf)
                print(f"    Upserted {len(buf)} (total {total_points}).")
                buf = [] # Reset the buffer.

    # After the loop, upsert any remaining points in the buffer.
    if buf:
        client.upsert(args.collection_name, buf, wait=True)
        total_points += len(buf)
        print(f"    Upserted final {len(buf)} (total {total_points}).")

    # --- Final Summary ---
    dt = time.time() - t0
    print("\n— Upsert complete —")
    if total_points > 0:
        avg = dt / total_points
        print(f"Inserted {total_points} points in {dt:.1f} s (avg {avg:.4f} s/pt).")
    else:
        print("No points were inserted.")

    # A final check to confirm the state of the collection in Qdrant.
    info = client.get_collection(args.collection_name)
    print(f"Collection status: {info.status}, points: {info.points_count}")
    print("Optimizer may run in background; status becomes GREEN when done.")


if __name__ == "__main__":
    main()