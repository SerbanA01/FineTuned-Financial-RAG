{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the same model used for indexing\n",
    "# Make sure this matches what generated your .npy files\n",
    "embedding_model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "try:\n",
    "    query_model = SentenceTransformer(embedding_model_name)\n",
    "    print(f\"Embedding model '{embedding_model_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading embedding model: {e}\")\n",
    "    # Handle error: model might not be downloaded, or path is wrong\n",
    "    query_model = None\n",
    "\n",
    "\n",
    "def get_query_embedding(query_text: str):\n",
    "    if not query_model:\n",
    "        raise ValueError(\"Embedding model not loaded.\")\n",
    "    # The model expects a list of texts, even if it's just one\n",
    "    #q_emb = enc.encode(question, normalize_embeddings=True).tolist()\n",
    "    q_emb = query_model.encode(query_text, normalize_embeddings=True).tolist()\n",
    "    return q_emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "QDRANT_URL_LOCAL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME_LOCAL = \"financial_sp500_local_final_v2\" # Your collection name\n",
    "client = QdrantClient(url=QDRANT_URL_LOCAL)\n",
    "\n",
    "def search_qdrant(query_vector, top_k=5):\n",
    "    if not client:\n",
    "        raise ValueError(\"Qdrant client not initialized.\")\n",
    "    if not query_vector:\n",
    "        raise ValueError(\"Query vector is empty.\")\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=COLLECTION_NAME_LOCAL,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k,  # Number of results to return\n",
    "        with_payload=True,  # Crucial: Get the original text and metadata\n",
    "        with_vectors=False # Usually not needed for RAG context\n",
    "    )\n",
    "    return search_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Assuming your previous code for query_model, client, get_query_embedding, search_qdrant is present)\n",
    "\n",
    "def format_retrieved_context(search_results, max_context_chars=15000):\n",
    "    \"\"\"\n",
    "    Formats the search results from Qdrant into a single string\n",
    "    to be used as context for the LLM.\n",
    "    Also returns a list of source metadata for citation.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    sources_metadata = []\n",
    "    current_chars = 0\n",
    "\n",
    "    if not search_results:\n",
    "        return \"\", []\n",
    "\n",
    "    for i, hit in enumerate(search_results):\n",
    "        payload = hit.payload\n",
    "        if payload:\n",
    "            chunk_text = payload.get(\"chunk_text\", \"\")\n",
    "            \n",
    "            # Estimate if adding this chunk will exceed the character limit\n",
    "            if current_chars + len(chunk_text) > max_context_chars and context_parts:\n",
    "                print(f\"Context character limit ({max_context_chars}) reached. Stopping context assembly.\")\n",
    "                break # Stop adding more chunks if limit is close\n",
    "\n",
    "            source_info = f\"Source {i+1}:\\n\"\n",
    "            source_info += f\"  Ticker: {payload.get('ticker', 'N/A')}\\n\"\n",
    "            source_info += f\"  Year: {payload.get('year', 'N/A')}\\n\"\n",
    "            source_info += f\"  Filing: {payload.get('filing_type', payload.get('filing_category', 'N/A'))}\\n\"\n",
    "            if payload.get('source_type') == 'sec_filing':  #need a check to see if that is what is called\n",
    "                source_info += f\"  Section: {payload.get('section_name', 'N/A')}\\n\"\n",
    "                source_info += f\"  Item: {payload.get('item', 'N/A')}\\n\"\n",
    "            elif payload.get('source_type') == 'earnings_transcript':\n",
    "                source_info += f\"  Quarter: {payload.get('quarter', 'N/A')}\\n\"\n",
    "                source_info += f\"  Speaker: {payload.get('turn_speaker_simple', 'N/A')}\\n\"\n",
    "                source_info += f\"  Call Section: {payload.get('turn_section', 'N/A')}\\n\"\n",
    "            # source_info += f\"  Original File: {payload.get('original_file_name', 'N/A')}\\n\" # Optional\n",
    "            # source_info += f\"  (Qdrant Score: {hit.score:.4f})\\n\" # Optional, for debugging relevance\n",
    "\n",
    "            context_parts.append(source_info)\n",
    "            context_parts.append(f\"  Content: {chunk_text}\\n---\\n\")\n",
    "            \n",
    "            sources_metadata.append(payload) # Store the full payload for more detailed citation if needed\n",
    "            current_chars += len(source_info) + len(chunk_text) + 5 # Rough estimate for newlines etc.\n",
    "        \n",
    "    return \"\".join(context_parts), sources_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "# IMPORTANT: Set your OpenAI API key as an environment variable:\n",
    "# export OPENAI_API_KEY=\"your_api_key_here\"\n",
    "# Or, pass it directly: client_openai = OpenAI(api_key=\"your_api_key_here\")\n",
    "try:\n",
    "    llm_client = OpenAI() # Reads API key from environment variable OPENAI_API_KEY\n",
    "    LLM_MODEL_NAME = \"gpt-3.5-turbo\" # Or \"gpt-4\", \"gpt-4-turbo-preview\", etc.\n",
    "    print(f\"OpenAI client initialized for model: {LLM_MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    print(\"Please ensure the 'openai' library is installed and your API key is set.\")\n",
    "    llm_client = None\n",
    "\n",
    "\n",
    "def get_llm_response(query_text: str, context_string: str):\n",
    "    if not llm_client:\n",
    "        raise ValueError(\"LLM client not initialized.\")\n",
    "\n",
    "    system_prompt = \"You are a helpful financial analyst assistant. Answer the user's question based ONLY on the provided context. If the information is not in the context, say you don't know or that the context doesn't provide the answer. Be concise and cite the sources if specific information is used.\"\n",
    "    \n",
    "    user_message = f\"\"\"\n",
    "Context from financial documents:\n",
    "---\n",
    "{context_string}\n",
    "---\n",
    "User Question: {query_text}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    try:\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=LLM_MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            temperature=0.2 # Lower temperature for more factual, less creative answers\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting response from LLM: {e}\")\n",
    "        return \"Error: Could not get a response from the LLM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05feba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_rag(user_query: str, top_k_retrieval=5):\n",
    "    print(f\"\\nProcessing query: '{user_query}'\")\n",
    "\n",
    "    # 1. Embed the query\n",
    "    print(\"Embedding user query...\")\n",
    "    try:\n",
    "        # For BGE models, add the recommended prefix for retrieval queries\n",
    "        if \"bge-\" in embedding_model_name.lower(): # embedding_model_name from your setup\n",
    "            query_for_embedding = f\"Represent this sentence for searching relevant passages: {user_query}\"\n",
    "        else:\n",
    "            query_for_embedding = user_query\n",
    "        \n",
    "        query_vector = get_query_embedding(query_for_embedding) # Use your embedding function\n",
    "        print(f\"Query embedded (first 5 dims): {query_vector[:5]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding query: {e}\")\n",
    "        return \"Could not process the query due to an embedding error.\"\n",
    "\n",
    "    # 2. Search Qdrant for relevant chunks\n",
    "    print(f\"Searching Qdrant for top {top_k_retrieval} relevant chunks...\")\n",
    "    try:\n",
    "        search_results = search_qdrant(query_vector, top_k=top_k_retrieval) # Use your Qdrant search\n",
    "        if not search_results:\n",
    "            print(\"No relevant documents found in Qdrant.\")\n",
    "            return \"I could not find relevant information in the documents to answer your question.\"\n",
    "        print(f\"Found {len(search_results)} potential chunks.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching Qdrant: {e}\")\n",
    "        return \"Could not process the query due to a database search error.\"\n",
    "\n",
    "    # 3. Format the retrieved context\n",
    "    print(\"Formatting context for LLM...\")\n",
    "    context_string, sources_metadata = format_retrieved_context(search_results)\n",
    "    if not context_string:\n",
    "        print(\"No context could be formatted (perhaps chunks were empty or too large).\")\n",
    "        return \"I found some documents, but could not prepare them to answer your question.\"\n",
    "    \n",
    "    # print(\"\\n--- Context being sent to LLM ---\")\n",
    "    # print(context_string[:1000] + \"...\" if len(context_string) > 1000 else context_string)\n",
    "    # print(\"--- End of Context ---\")\n",
    "\n",
    "    # 4. Get response from LLM\n",
    "    print(\"Getting response from LLM...\")\n",
    "    try:\n",
    "        llm_answer = get_llm_response(user_query, context_string)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM call: {e}\")\n",
    "        return \"An error occurred while trying to generate an answer with the LLM.\"\n",
    "        \n",
    "    # 5. Return the answer (and optionally sources)\n",
    "    # You can enhance this to return structured output\n",
    "    print(\"\\n--- RAG Process Complete ---\")\n",
    "    return llm_answer, sources_metadata\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not LIBRARIES_AVAILABLE or not query_model or not client or not llm_client:\n",
    "        print(\"Exiting: One or more essential components (libraries, models, clients) failed to initialize.\")\n",
    "    else:\n",
    "        # Test queries\n",
    "        queries = [\n",
    "            \"What were Agilent's main business segments in 2018?\",\n",
    "            \"What did Agilent say about the COVID-19 impact in early 2020?\",\n",
    "            \"Summarize the key risks for a major tech company in their 2023 10-K.\",\n",
    "            \"What was Apple's revenue in their latest reported quarter for which data exists?\" # This requires up-to-date data\n",
    "        ]\n",
    "\n",
    "        for q in queries:\n",
    "            answer, sources = answer_query_with_rag(q, top_k_retrieval=3) # Get 3 chunks for context\n",
    "            print(f\"\\n\\nQuery: {q}\")\n",
    "            print(f\"LLM Answer:\\n{answer}\")\n",
    "            \n",
    "            # print(\"\\nSources Used (Payloads):\")\n",
    "            # for i, src_meta in enumerate(sources):\n",
    "            #     print(f\"  Source {i+1} Ticker: {src_meta.get('ticker')}, File: {src_meta.get('original_file_name')}, Section: {src_meta.get('section_name', src_meta.get('turn_section'))}\")\n",
    "            print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
