{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWYQ6uOgwG2C",
        "outputId": "d363fe0d-10c9-4275-eabd-cbe297c077f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sec-edgar-downloader in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.0.3)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: requests in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sec-edgar-downloader) (2.32.3)\n",
            "Requirement already satisfied: pyrate-limiter>=3.6.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sec-edgar-downloader) (3.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->sec-edgar-downloader) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->sec-edgar-downloader) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->sec-edgar-downloader) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->sec-edgar-downloader) (2024.12.14)\n",
            "Requirement already satisfied: investpy in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.8)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: Unidecode>=1.1.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from investpy) (1.4.0)\n",
            "Requirement already satisfied: setuptools>=41.2.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from investpy) (75.8.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from investpy) (2.1.3)\n",
            "Requirement already satisfied: pandas>=0.25.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from investpy) (2.2.3)\n",
            "Requirement already satisfied: lxml>=4.4.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from investpy) (5.4.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from investpy) (2.32.3)\n",
            "Requirement already satisfied: pytz>=2019.3 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from investpy) (2025.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.25.1->investpy) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=0.25.1->investpy) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.22.0->investpy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.22.0->investpy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.22.0->investpy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.22.0->investpy) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.1->investpy) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aiohttp in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.11.18)\n",
            "Requirement already satisfied: aiofiles in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (24.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.12.3)\n",
            "Requirement already satisfied: PyPDF2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
            "Requirement already satisfied: langchain in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.25)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp) (1.20.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aiolimiter in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nest_asyncio in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (1.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: langdetect in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.9)\n",
            "Requirement already satisfied: click in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.61)\n",
            "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.31 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (0.11.1)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (6.31.0)\n",
            "Requirement already satisfied: websockets>=13.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.3.1)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from ftfy) (0.2.13)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aiofiles in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (24.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\serban\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\serban\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Serban\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "%pip install sec-edgar-downloader\n",
        "%pip install investpy\n",
        "%pip install PyPDF2\n",
        "%pip install aiohttp aiofiles beautifulsoup4 PyPDF2 langchain\n",
        "%pip install aiolimiter\n",
        "%pip install nest_asyncio\n",
        "%pip install tqdm\n",
        "%pip install nltk langdetect\n",
        "%pip install yfinance\n",
        "%pip install ftfy\n",
        "%pip install aiofiles\n",
        "%pip install nltk\n",
        "!python -m nltk.downloader punkt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yrW-VE2KYvOM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import logging\n",
        "import pickle\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from pydantic import BaseModel, ValidationError\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from io import StringIO, BytesIO\n",
        "import investpy\n",
        "import json\n",
        "from typing import List, Dict, Optional\n",
        "import gzip\n",
        "import sqlite3\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from dateutil import parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DxwEKgZP3K5g"
      },
      "outputs": [],
      "source": [
        "#maybe consider FTSE100\n",
        "#don t forget about cryptos and forex\n",
        "\n",
        "def get_sp500_tickers():\n",
        "    \"\"\"Get S&P 500 tickers with more reliable table parsing\"\"\"\n",
        "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    table = soup.find('table', {'id': 'constituents'})\n",
        "    df = pd.read_html(StringIO(str(table)))[0]\n",
        "    return df['Symbol'].tolist()\n",
        "\n",
        "def get_nasdaq_tickers():\n",
        "    \"\"\"Get NASDAQ-listed common stocks with proper column handling\"\"\"\n",
        "    url = \"https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqlisted.txt\"\n",
        "    columns = [\n",
        "        'Symbol', 'Security Name', 'Market Category', 'Test Issue',\n",
        "        'Financial Status', 'Round Lot Size', 'ETF', 'NextShares'\n",
        "    ]\n",
        "    df = pd.read_csv(url, sep=\"|\", names=columns)\n",
        "    df = df[:-1]  # Remove summary row\n",
        "    # Filter out ETFs and test issues\n",
        "    df = df[(df['Test Issue'] == 'N') & (df['ETF'] == 'N')]\n",
        "    return df['Symbol'].tolist()\n",
        "\n",
        "def get_other_tickers():\n",
        "    \"\"\"Get NYSE/AMEX stocks with proper column handling\"\"\"\n",
        "    url = \"https://www.nasdaqtrader.com/dynamic/SymDir/otherlisted.txt\"\n",
        "    columns = [\n",
        "        'ACT Symbol', 'Security Name', 'Exchange', 'CQS Symbol',\n",
        "        'ETF', 'Round Lot Size', 'Test Issue', 'NASDAQ Symbol'\n",
        "    ]\n",
        "    df = pd.read_csv(url, sep=\"|\", names=columns)\n",
        "    df = df[:-1]  # Remove summary row\n",
        "    # Filter out ETFs and test issues\n",
        "    df = df[(df['Test Issue'] == 'N') & (df['ETF'] == 'N')]\n",
        "    return df['ACT Symbol'].tolist()\n",
        "\n",
        "def get_global_tickers():\n",
        "    \"\"\"Get global tickers with error handling\"\"\"\n",
        "    try:\n",
        "        stocks_df = investpy.stocks.get_stocks()\n",
        "        # Filter for common stock types (adjust based on available data)\n",
        "        if 'type' in stocks_df.columns:\n",
        "            stocks_df = stocks_df[stocks_df['type'] == 'Stock']\n",
        "        return stocks_df['symbol'].unique().tolist()\n",
        "    except ImportError:\n",
        "        print(\"Global tickers disabled: install investpy (pip install investpy)\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching global tickers: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def get_all_tickers():\n",
        "    \"\"\"Combine all sources with error handling\"\"\"\n",
        "    tickers = []\n",
        "\n",
        "    try:\n",
        "        tickers += get_sp500_tickers()\n",
        "        print(f\"Found {len(tickers)} S&P 500 tickers\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching S&P 500: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        nasdaq = get_nasdaq_tickers()\n",
        "        tickers += nasdaq\n",
        "        print(f\"Added {len(nasdaq)} NASDAQ tickers\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching NASDAQ: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        other = get_other_tickers()\n",
        "        tickers += other\n",
        "        print(f\"Added {len(other)} NYSE/AMEX tickers\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching NYSE/AMEX: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        global_tickers = get_global_tickers()\n",
        "        tickers += global_tickers\n",
        "        print(f\"Added {len(global_tickers)} global tickers\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching global tickers: {str(e)}\")\n",
        "\n",
        "    # Clean and deduplicate\n",
        "    clean_tickers = list(set([t.strip().upper() for t in tickers if isinstance(t, str)]))\n",
        "    return sorted(clean_tickers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IEplcgE_zgV",
        "outputId": "ba833273-b0eb-4b77-b0f0-4c2191b874cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 503 S&P 500 tickers\n",
            "Added 4002 NASDAQ tickers\n",
            "Added 3158 NYSE/AMEX tickers\n",
            "Global tickers disabled: install investpy (pip install investpy)\n",
            "Added 0 global tickers\n",
            "\n",
            "Total unique tickers: 7159\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    all_tickers = get_all_tickers()\n",
        "    print(f\"\\nTotal unique tickers: {len(all_tickers)}\")\n",
        "\n",
        "\n",
        "\n",
        "    #!!! separate the global tickers from the important markets so that the code for the filings are done on the important markets\n",
        "\n",
        "\n",
        "\n",
        "    # Save to file\n",
        "    # with open(\"all_tickers.txt\", \"w\") as f:\n",
        "    #     f.write(\"\\n\".join(all_tickers))\n",
        "\n",
        "    #store_historical_data()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bTrgtkci7Ki"
      },
      "source": [
        "**The above code works well for retrieving sec filings, whena query is prompted, the model will call the retriver for the sec filings which will be stored in a database either loccal or on the cloud if the sec filings is what is needed, or it will either call the api for the stock price or the api for the analytics- Finnhub**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f8JJMMVwpp8"
      },
      "outputs": [],
      "source": [
        "# --- Test Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    filings = fetch_sec_filings(\"AAPL\", filing_type='10-Q' ,max_results=1,save_to_disk=False)\n",
        "    print(filings)\n",
        "    print(filings[0]['documents'][0]['text'][2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V7lwcU_qQ1Hi"
      },
      "outputs": [],
      "source": [
        "import os, re, json, time, asyncio, aiofiles\n",
        "from io import BytesIO\n",
        "from typing import List\n",
        "from bs4 import BeautifulSoup\n",
        "from PyPDF2 import PdfReader\n",
        "import aiohttp\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- CONFIG ---\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"University_Project_Financial_LLM (muie.restante@gmail.com)\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate\",\n",
        "    \"Host\": \"www.sec.gov\"\n",
        "}\n",
        "SEC_BASE = \"https://www.sec.gov\"\n",
        "CIK_LOOKUP = f\"{SEC_BASE}/files/company_tickers.json\"\n",
        "SEARCH_URL = f\"{SEC_BASE}/cgi-bin/browse-edgar\"\n",
        "DATA_DIR = \"sec_filings_second\"\n",
        "DELAY = 1.0\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "MAX_CONCURRENT_REQUESTS = 5\n",
        "\n",
        "# --- UTILS ---\n",
        "async def async_get(session, url, semaphore, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        async with semaphore:\n",
        "            try:\n",
        "                async with session.get(url, headers=HEADERS, timeout=aiohttp.ClientTimeout(total=30)) as resp:\n",
        "                    await asyncio.sleep(DELAY)\n",
        "                    if resp.status == 429:\n",
        "                        wait_time = 2 ** (attempt + 5)\n",
        "                        print(f\"Rate limited. Retrying in {wait_time}s...\")\n",
        "                        await asyncio.sleep(wait_time)\n",
        "                        continue\n",
        "                    content_type = resp.headers.get(\"Content-Type\", \"\")\n",
        "                    if \"application/json\" not in content_type and url.endswith(\".json\"):\n",
        "                        print(f\"Unexpected content type: {content_type}\")\n",
        "                        return None\n",
        "                    return await resp.read()\n",
        "            except Exception as e:\n",
        "                print(f\"Request failed (attempt {attempt+1}): {e}\")\n",
        "                if attempt == retries - 1:\n",
        "                    return None\n",
        "                await asyncio.sleep(2 ** (attempt + 1))\n",
        "    return None\n",
        "\n",
        "async def download_cik_lookup(session):\n",
        "    try:\n",
        "        async with session.get(CIK_LOOKUP, headers=HEADERS) as resp:\n",
        "            await asyncio.sleep(DELAY)\n",
        "            if resp.status != 200:\n",
        "                raise Exception(f\"Status {resp.status}\")\n",
        "            content_type = resp.headers.get(\"Content-Type\", \"\")\n",
        "            if \"application/json\" not in content_type:\n",
        "                raise Exception(f\"Unexpected content-type: {content_type}\")\n",
        "            return await resp.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch CIK lookup: {e}\")\n",
        "        return {}\n",
        "\n",
        "def get_cik_from_lookup(ticker: str, lookup_data: dict) -> str:\n",
        "    for entry in lookup_data.values():\n",
        "        if entry[\"ticker\"].lower() == ticker.lower():\n",
        "            return str(entry[\"cik_str\"]).zfill(10)\n",
        "    return None\n",
        "\n",
        "def split_text(text: str) -> List[str]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "    text = re.sub(r\"UNITED STATES SECURITIES.*?Washington, D\\.C\\.\\s*\\d+\", \"\", text, flags=re.DOTALL)\n",
        "    text = re.sub(r\"Table of Contents.*?(?=Item\\s+1)\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
        "    text = re.sub(r'[0-9]{10,}', '', text)\n",
        "    text = re.sub(r'[a-fA-F0-9]{32,}', '', text)\n",
        "    return re.sub(r'\\n+', '\\n', text.replace(\"\\xa0\", \" \")).strip()\n",
        "\n",
        "async def get_filing_urls(session, cik: str, filing_type: str, max_docs=5):\n",
        "    try:\n",
        "        params = {\n",
        "            \"action\": \"getcompany\",\n",
        "            \"CIK\": cik,\n",
        "            \"type\": filing_type,\n",
        "            \"owner\": \"exclude\",\n",
        "            \"count\": \"100\"\n",
        "        }\n",
        "        async with session.get(SEARCH_URL, params=params, headers=HEADERS) as resp:\n",
        "            content = await resp.text()\n",
        "            soup = BeautifulSoup(content, \"html.parser\")\n",
        "            table = soup.find(\"table\", class_=\"tableFile2\")\n",
        "            filings = []\n",
        "            if table:\n",
        "                rows = table.find_all(\"tr\")[1:]\n",
        "                for row in rows:\n",
        "                    cols = row.find_all(\"td\")\n",
        "                    if len(cols) >= 4:\n",
        "                        link = cols[1].find(\"a\")\n",
        "                        date = cols[3].text.strip()\n",
        "                        if link:\n",
        "                            filings.append({\n",
        "                                \"detail_url\": f\"{SEC_BASE}{link['href']}\",\n",
        "                                \"date\": date\n",
        "                            })\n",
        "                        if len(filings) >= max_docs:\n",
        "                            break\n",
        "            return filings\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching filings for CIK {cik}: {e}\")\n",
        "        return []\n",
        "\n",
        "async def extract_text_from_doc(session, doc_url: str, semaphore) -> str:\n",
        "    raw = await async_get(session, doc_url, semaphore)\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    try:\n",
        "        if doc_url.endswith(\".pdf\"):\n",
        "            reader = PdfReader(BytesIO(raw))\n",
        "            text = \"\\n\".join(p.extract_text() or \"\" for p in reader.pages)\n",
        "        else:\n",
        "            soup = BeautifulSoup(raw, \"html.parser\")\n",
        "            text = soup.get_text()\n",
        "        return clean_text(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {doc_url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "async def process_filing(session, filing, ticker, filing_type, semaphore):\n",
        "    try:\n",
        "        raw = await async_get(session, filing[\"detail_url\"], semaphore)\n",
        "        if not raw:\n",
        "            return None\n",
        "        soup = BeautifulSoup(raw, \"html.parser\")\n",
        "        table = soup.find(\"table\", class_=\"tableFile\")\n",
        "        if not table:\n",
        "            return None\n",
        "        for row in table.find_all(\"tr\")[1:]:\n",
        "            cols = row.find_all(\"td\")\n",
        "            if len(cols) >= 4:\n",
        "                doc_type = cols[3].text.strip().lower()\n",
        "                href = cols[2].find(\"a\")\n",
        "                if not href:\n",
        "                    continue\n",
        "                href = href[\"href\"]\n",
        "                filename = href.split(\"/\")[-1].lower()\n",
        "                if(\n",
        "                    doc_type == filing_type.lower() and\n",
        "                    (filename.endswith(\".htm\") or filename.endswith(\".html\"))\n",
        "                    and not any(x in filename for x in [\"xbrl\", \"xml\", \"ex-99\", \"exhibit\", \"form\", \"index\"])\n",
        "                ):\n",
        "                    doc_url = f\"{SEC_BASE}{href}\".replace(\"/ix?doc=\", \"\") # Corrected line\n",
        "                    text = await extract_text_from_doc(session, doc_url, semaphore)\n",
        "                    if not text:\n",
        "                        continue\n",
        "                    return {\n",
        "                        \"ticker\": ticker,\n",
        "                        \"date\": filing[\"date\"],\n",
        "                        \"text\": text,\n",
        "                        \"url\": doc_url,\n",
        "                        \"filing_type\": filing_type\n",
        "                    }\n",
        "                        \n",
        "                    \n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing filing {filing['detail_url']}: {e}\")\n",
        "        return None\n",
        "\n",
        "async def process_company(session, ticker, cik, semaphore, filing_type=\"10-K\", max_docs=5):\n",
        "    filing_dir = os.path.join(DATA_DIR, ticker, filing_type)\n",
        "    if os.path.exists(filing_dir) and os.listdir(filing_dir):\n",
        "        print(f\"Skipping existing: {ticker} {filing_type}\")\n",
        "        return\n",
        "\n",
        "    filings = await get_filing_urls(session, cik, filing_type, max_docs)\n",
        "    if not filings:\n",
        "        print(f\"No filings found for {ticker} {filing_type}\")\n",
        "        return\n",
        "\n",
        "    for filing in filings:\n",
        "        path = os.path.join(DATA_DIR, ticker, filing_type, f\"{filing['date']}.json\")\n",
        "        if os.path.exists(path):\n",
        "            continue\n",
        "        doc = await process_filing(session, filing, ticker, filing_type, semaphore)\n",
        "        if doc:\n",
        "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "            async with aiofiles.open(path, \"w\") as f:\n",
        "                await f.write(json.dumps(doc))\n",
        "\n",
        "async def run_pipeline(tickers):\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        cik_lookup = await download_cik_lookup(session)\n",
        "        if not cik_lookup:\n",
        "            print(\"Aborting: Failed to fetch CIK lookup\")\n",
        "            return\n",
        "\n",
        "        BATCH_SIZE = 50\n",
        "        for i in range(0, len(tickers), BATCH_SIZE):\n",
        "            batch = tickers[i:i+BATCH_SIZE]\n",
        "            print(f\"Processing batch {i//BATCH_SIZE + 1}/{(len(tickers)//BATCH_SIZE)+1}\")\n",
        "            tasks = []\n",
        "            for ticker in batch:\n",
        "                cik = get_cik_from_lookup(ticker, cik_lookup)\n",
        "                if not cik:\n",
        "                    print(f\"CIK not found: {ticker}\")\n",
        "                    continue\n",
        "                tasks.append(process_company(session, ticker, cik, semaphore, \"10-K\", 5))\n",
        "                tasks.append(process_company(session, ticker, cik, semaphore, \"10-Q\", 15))\n",
        "            await asyncio.gather(*tasks)\n",
        "            print(f\"Finished batch {i//BATCH_SIZE + 1}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tickers = get_sp500_tickers() + get_nasdaq_tickers() + get_other_tickers()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts0il_EtgqLp",
        "outputId": "2692af63-4862-4d62-9fc5-867821b1ae1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7637\n"
          ]
        }
      ],
      "source": [
        "print(tickers.__len__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AaaHPhfPcfra"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1/1\n",
            "Finished batch 1\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "await run_pipeline(tickers[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9290DMNr19f"
      },
      "outputs": [],
      "source": [
        "SCRAPING_FILE = \"/content/drive/MyDrive/Licenta/earnings_transcripts/motley-fool-data.pkl\"\n",
        "DELAY = 1.0\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "def read_and_clean(scraping_file: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load a pickled Motley Fool earnings-call dump and return a cleaned DataFrame\n",
        "    suitable for downstream chunking / RAG ingestion.\n",
        "    \"\"\"\n",
        "\n",
        "    # ───────────────────────────────\n",
        "    # helpers\n",
        "    # ───────────────────────────────\n",
        "    def clean_date(raw: str) -> str:\n",
        "        \"\"\"Parse a date string and return ISO-8601 format.\"\"\"\n",
        "        if not isinstance(raw, str) or not raw.strip():\n",
        "            return raw\n",
        "        try:\n",
        "            # Remove trailing timezone (like \"ET\")\n",
        "            no_tz = re.sub(r\"\\s+[A-Z]{2,4}$\", \"\", raw.strip())\n",
        "            dt = parser.parse(no_tz, fuzzy=True)\n",
        "            return dt.isoformat()\n",
        "        except (ValueError, OverflowError) as exc:\n",
        "            print(f\"[WARN] Could not parse date “{raw}”: {exc}\")\n",
        "            return raw  # fallback to original if cannot parse\n",
        "\n",
        "    dash_re = re.compile(r\"\\s*--\\s*\")\n",
        "\n",
        "    def normalize_label(line: str) -> str:\n",
        "        if \"--\" in line:\n",
        "            return \" - \".join(part.strip() for part in dash_re.split(line))\n",
        "        return line\n",
        "\n",
        "    junk_hdr_re = re.compile(\n",
        "        r\"^(duration:|call participants:|questions and answers:|more .* analysis$|all earnings call transcripts$)\",\n",
        "        re.I,\n",
        "    )\n",
        "\n",
        "    def clean_transcript(blob: str) -> str:\n",
        "        \"\"\"Clean the transcript text.\"\"\"\n",
        "        out, prev_blank = [], False\n",
        "        for raw in blob.splitlines():\n",
        "            line = raw.rstrip()\n",
        "            if junk_hdr_re.match(line):\n",
        "                continue\n",
        "            if not line:\n",
        "                if not prev_blank:\n",
        "                    out.append(\"\")\n",
        "                prev_blank = True\n",
        "                continue\n",
        "            prev_blank = False\n",
        "            out.append(normalize_label(line))\n",
        "        return \"\\n\".join(out).strip()\n",
        "\n",
        "    def extract_participants(text: str) -> list:\n",
        "        names = set()\n",
        "        for line in text.splitlines():\n",
        "            if \" - \" not in line:\n",
        "                continue\n",
        "\n",
        "            name = line.split(\" - \")[0].strip()\n",
        "\n",
        "            if (\n",
        "                name.startswith(\"[\")                     # tags like [Foreign Speech]\n",
        "                or \"foreign speech\" in name.lower()      # safety net\n",
        "                or name.lower() in {\"operator\", \"analyst\"}\n",
        "                or len(name) < 2 or len(name) > 60\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            names.add(name)\n",
        "\n",
        "        return sorted(names)\n",
        "\n",
        "    # ───────────────────────────────\n",
        "    # load raw pickle → DataFrame\n",
        "    # ───────────────────────────────\n",
        "    with open(scraping_file, \"rb\") as fh:\n",
        "        raw_data = pickle.load(fh)\n",
        "\n",
        "    df = pd.DataFrame(raw_data).dropna(subset=[\"transcript\"])\n",
        "\n",
        "    # ───────────────────────────────\n",
        "    # clean fields\n",
        "    # ───────────────────────────────\n",
        "    df[\"date\"] = df[\"date\"].apply(clean_date)\n",
        "    df[\"exchange\"] = df[\"exchange\"].str.split(\":\").str[0].str.upper().str.strip()\n",
        "    df[\"transcript\"] = df[\"transcript\"].apply(clean_transcript)\n",
        "    df[\"year\"] = df[\"q\"].str.extract(r\"(\\d{4})\")\n",
        "    df[\"quarter\"] = df[\"q\"].str.extract(r\"Q([1-4])\")\n",
        "    df[\"document_type\"] = \"earnings_call\"\n",
        "    df[\"language\"] = \"en\"\n",
        "    df[\"participants\"] = df[\"transcript\"].apply(extract_participants)\n",
        "\n",
        "    df = df[\n",
        "        [\n",
        "            \"date\",\n",
        "            \"year\",\n",
        "            \"quarter\",\n",
        "            \"exchange\",\n",
        "            \"ticker\",\n",
        "            \"q\",\n",
        "            \"document_type\",\n",
        "            \"language\",\n",
        "            \"participants\",\n",
        "            \"transcript\",\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "    # ───────────────────────────────\n",
        "    # sanity checks\n",
        "    # ───────────────────────────────\n",
        "    if df[\"transcript\"].isnull().any():\n",
        "        raise ValueError(\"Null transcripts detected after cleaning.\")\n",
        "    if len(df) != len(raw_data):\n",
        "        raise ValueError(\"Row count mismatch after cleaning.\")\n",
        "\n",
        "    print(f\"✅ Cleaned {len(df)} records · columns → {list(df.columns)}\")\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61QIn02U6ggN"
      },
      "outputs": [],
      "source": [
        "cleaned = read_and_clean(SCRAPING_FILE)\n",
        "for index, row in cleaned.head(2).iterrows():\n",
        "    for column_name, value in row.items():\n",
        "        print(f\"{column_name}: {value}\")\n",
        "    print(\"\\n\")  # Add an extra newline for readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqft4afC2p13",
        "outputId": "732357c8-0fde-436a-8385-3244ccb8971b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Cleaned 18755 records · columns → ['date', 'year', 'quarter', 'exchange', 'ticker', 'q', 'document_type', 'language', 'participants', 'transcript']\n"
          ]
        }
      ],
      "source": [
        "#chunk and store the data\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
        ")\n",
        "\n",
        "all_chunks = []\n",
        "\n",
        "cleaned = read_and_clean(SCRAPING_FILE)\n",
        "for _, row in cleaned.iterrows():\n",
        "    for chunk in splitter.split_text(row[\"transcript\"]):\n",
        "        all_chunks.append(\n",
        "            {\n",
        "                \"text_chunks\": chunk,\n",
        "                \"metadata\": {\n",
        "                    \"ticker\": row.ticker,\n",
        "                    \"date\": row.date,\n",
        "                    \"year\": row.year,\n",
        "                    \"quarter\": row.quarter,\n",
        "                    \"participants\": row.participants,\n",
        "                }\n",
        "            }\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "MPcbo7HD7563",
        "outputId": "87a81c39-bd55-4bc7-b005-e7fe702fb761"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Licenta/earnings_transcripts/companies\"\n",
        "\n",
        "# Group chunks by ticker+year+quarter\n",
        "transcript_map = {}\n",
        "for chunk in all_chunks:\n",
        "    meta = chunk['metadata']\n",
        "    key = f\"{meta['ticker']}_{meta['year']}_Q{meta['quarter']}\"\n",
        "\n",
        "    if key not in transcript_map:\n",
        "        transcript_map[key] = {\n",
        "            \"metadata\": {\n",
        "                \"ticker\": meta['ticker'],\n",
        "                \"date\": meta['date'],\n",
        "                \"year\": meta['year'],\n",
        "                \"quarter\": meta['quarter'],\n",
        "                \"participants\": meta['participants']\n",
        "            },\n",
        "            \"chunks\": []\n",
        "        }\n",
        "\n",
        "    # Just add chunk text with ID\n",
        "    transcript_map[key]['chunks'].append({\n",
        "        \"id\": len(transcript_map[key]['chunks']) + 1,\n",
        "        \"text\": chunk['text_chunks']\n",
        "    })\n",
        "\n",
        "# Save each group as a separate file\n",
        "for key, transcript in transcript_map.items():\n",
        "    ticker = transcript['metadata']['ticker']\n",
        "    year = transcript['metadata']['year']\n",
        "    quarter = transcript['metadata']['quarter']\n",
        "\n",
        "    # Create company directory\n",
        "    company_dir = os.path.join(SAVE_DIR, ticker)\n",
        "    os.makedirs(company_dir, exist_ok=True)\n",
        "\n",
        "    # Save as JSON\n",
        "    filename = f\"{ticker}_{year}_Q{quarter}.json\"\n",
        "    with open(os.path.join(company_dir, filename), 'w') as f:\n",
        "        json.dump(transcript, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjzZQsyd5HO-",
        "outputId": "e3e2cde2-702c-4271-f6c7-23fcb0c054b7"
      },
      "outputs": [],
      "source": [
        "for i, chunk in enumerate(all_chunks):\n",
        "  print(f\"Chunk {i + 1}:\")\n",
        "  print(\"-\" * 20)\n",
        "  print(f\"Content: {chunk['text_chunks']}\")\n",
        "  print(\"-\" * 20)\n",
        "  print(\"Metadata:\")\n",
        "  for key, value in chunk['metadata'].items():  # Iterate through metadata\n",
        "    print(f\"  {key}: {value}\")  # Print key-value pairs\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This worked !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "import re, json, ftfy, os\n",
        "from pathlib import Path\n",
        "from nltk.tokenize import sent_tokenize   # run `python -m nltk.downloader punkt` once\n",
        "from dateutil import parser\n",
        "import logging\n",
        "\n",
        "# ---------- 1. header/footer patterns ----------\n",
        "HEADER_FOOTER_PATTERNS = [\n",
        "    re.compile(r'^\\s*TABLE OF CONTENTS\\s*$',                 re.I),\n",
        "    re.compile(r'^\\s*INDEX TO FINANCIAL STATEMENTS\\s*$',     re.I),\n",
        "    re.compile(r'^\\s*F-\\d+\\s*$|^\\s*\\d+\\s*$',                 re.I),          # “F-3” or bare page numbers\n",
        "    re.compile(r'^\\s*(10-K|10-Q)\\b.*$',                      re.I),          # “10-K 1 …”\n",
        "    re.compile(r'^\\s*Item\\s+\\d+[A-Z]?\\.?.*\\s+\\d+\\s*$',       re.I),          # “Item 1. Business 1”\n",
        "    re.compile(r'^\\s*THE ORIGINAL BARK COMPANY.*$',          re.I),          # company-specific header\n",
        "]\n",
        "\n",
        "def strip_headers_footers(text: str) -> str:\n",
        "    lines = []\n",
        "    for ln in text.splitlines():\n",
        "        if any(pat.match(ln) for pat in HEADER_FOOTER_PATTERNS):\n",
        "            continue\n",
        "        lines.append(ln)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# ---------- 2. sentence-level cleaner ----------\n",
        "def clean_text(text: str) -> str:\n",
        "    text = ftfy.fix_text(text.replace('�', \"'\").replace('’', \"'\"))\n",
        "\n",
        "    # split camelCase but keep “U.S.”\n",
        "    text = re.sub(r'(?<=[a-z])(?=[A-Z][a-z])', ' ', text)\n",
        "\n",
        "    # collapse whitespace\n",
        "    text = re.sub(r'[\\n\\r\\t]+', ' ', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
        "    #added the item 1 strip!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    \n",
        "    # sentence split (NLTK if available, regex otherwise)\n",
        "    sents = sent_tokenize(text) if sent_tokenize else re.split(r'(?<=[.!?])\\s+(?=[A-Z(])', text)\n",
        "\n",
        "    dedup, seen = [], set()\n",
        "    for s in sents:\n",
        "        norm = re.sub(r'\\s+', ' ', s).strip().lower()\n",
        "        if norm and norm not in seen:\n",
        "            dedup.append(s.strip())\n",
        "            seen.add(norm)\n",
        "\n",
        "    return ' '.join(dedup)\n",
        "\n",
        "# ---------- 3. main one-file cleaner ----------\n",
        "def process_file(path: Path, dest_root: Path):\n",
        "    data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    chunks = data.get(\"text_chunks\", [])\n",
        "    if data.get(\"filing_type\") == \"10-K\":\n",
        "        chunks = chunks[1:]                     # drop SEC header chunk\n",
        "\n",
        "    # -------- cross-chunk de-duplication -----------------------------------\n",
        "    joined, seen = [], set()\n",
        "    for ch in chunks:\n",
        "        if not ch or \"us-gaap\" in ch.lower():\n",
        "            continue\n",
        "        norm = re.sub(r'\\s+', ' ', ch).strip().lower()\n",
        "        if norm in seen:\n",
        "            continue\n",
        "        seen.add(norm)\n",
        "        joined.append(ch.strip())\n",
        "\n",
        "    full_text = ' '.join(joined)\n",
        "\n",
        "    # -------- hard-cut everything before “ITEM 1.” -------------------------\n",
        "    # m = re.search(r'(?i)\\bITEM\\s+1\\.\\s', full_text)\n",
        "    m = re.search(\n",
        "        r'''(?is)                 # i = ignore-case, s = dot matches newline\n",
        "            Item                  # literally “Item” (no word-boundary)\n",
        "            \\s*1                  # spaces (optional) then the digit 1\n",
        "            \\s*[\\.\\u2022]         # a dot or the bullet ● some filings use\n",
        "            (?!\\d)                # NOT followed by another digit (skips Item 10.)\n",
        "        ''',\n",
        "        full_text,\n",
        "        re.VERBOSE\n",
        "        )\n",
        "    if m:\n",
        "        full_text = full_text[m.start():]\n",
        "    else:\n",
        "        print(f\"Warning: 'Item 1.' not found in {path.name}\")\n",
        "\n",
        "    full_text = strip_headers_footers(full_text)\n",
        "    full_text = clean_text(full_text)\n",
        "\n",
        "    # -------- drop tiny leftovers -----------------------------------------\n",
        "    MIN_WORDS = 50\n",
        "    if len(full_text.split()) < MIN_WORDS:\n",
        "        logging.info(\"Dropped %s (only %d words)\", path, len(full_text.split()))\n",
        "        return                         # nothing written\n",
        "\n",
        "    # =======================  METADATA BLOCK  ==============================\n",
        "    date_str = data.get(\"date\")                       # e.g. \"2022-12-21\"\n",
        "    try:\n",
        "        year_of_filing = parser.parse(date_str).year if date_str else None\n",
        "    except Exception as e:\n",
        "        print(f\"Could not parse year from {date_str}: {e}\")\n",
        "        year_of_filing = None\n",
        "\n",
        "    source_url   = data.get(\"url\")                    # e.g. \".../data/1090872/...\"\n",
        "    cik_match    = re.search(r\"/data/(\\d+)/\", source_url or \"\")\n",
        "    extracted_cik = cik_match.group(1).zfill(10) if cik_match else None\n",
        "\n",
        "    out = {\n",
        "        \"cleaned_text\": full_text,\n",
        "        \"ticker\":      data.get(\"ticker\"),\n",
        "        \"date\":        date_str,\n",
        "        \"year\":        year_of_filing,\n",
        "        \"cik\":         extracted_cik,\n",
        "        \"filing_type\": data.get(\"filing_type\"),\n",
        "    }\n",
        "    # ======================================================================\n",
        "\n",
        "    # mirror raw-path subtree under sec_filings_clean/\n",
        "    out_path = dest_root / path.relative_to(RAW_ROOT)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2),\n",
        "                        encoding=\"utf-8\")\n",
        "\n",
        "# ---------- 4. batch driver ----------\n",
        "# -----------------------------------------\n",
        "# 5.  ultra-simple batch driver (no Pool) \n",
        "# # -----------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    import glob, sys\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    RAW_ROOT   = Path(\"sec_filings\")\n",
        "    CLEAN_ROOT = Path(\"sec_filings_clean\")\n",
        "\n",
        "    files = glob.glob(str(RAW_ROOT / \"**/*.json\"), recursive=True)\n",
        "    if not files:\n",
        "        sys.exit(\"No raw filings found under sec_filings/\")\n",
        "\n",
        "    errors = 0\n",
        "    for fp in tqdm(files, desc=\"Cleaning filings\"):\n",
        "        try:\n",
        "            process_file(Path(fp), CLEAN_ROOT)\n",
        "            print(f\"Cleaned: {fp}\")\n",
        "        except Exception as e:\n",
        "            errors += 1\n",
        "            with open(\"clean_errors.log\", \"a\", encoding=\"utf-8\") as log:\n",
        "                log.write(f\"{fp}\\t{e}\\n\")\n",
        "\n",
        "    print(f\"\\nDone.  {len(files)-errors} cleaned, {errors} errors (see clean_errors.log)\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching CIK lookup data...\n",
            "Successfully fetched 10039 CIKs.\n",
            "Working with 7880 unique CIKs.\n",
            "DEBUG: Using a subset of 1 CIKs for processing.\n",
            "Failed to fetch master file: Status 403\n",
            "No tasks to process based on the master file and CIK list.\n"
          ]
        }
      ],
      "source": [
        "import gzip, csv, itertools, aiohttp, asyncio, re, json, os, textwrap\n",
        "\n",
        "FORM_TYPES  = {\"10-K\", \"10-Q\"}\n",
        "\n",
        "# Define DATA_ROOT, used in fetch_and_store\n",
        "DATA_ROOT = \"test\" # Or your desired path\n",
        "\n",
        "# --- CONFIG for CIK lookup ---\n",
        "CIK_LOOKUP_URL = \"https://www.sec.gov/files/company_tickers.json\"\n",
        "LOOKUP_HEADERS = {\n",
        "    \"User-Agent\": \"University_Project_Financial_LLM (muie.restante@gmail.com)\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate\",\n",
        "    \"Host\": \"www.sec.gov\"\n",
        "}\n",
        "DELAY_LOOKUP = 0.1 # Adjusted delay for a single request\n",
        "\n",
        "async def download_ciks(session):\n",
        "    \"\"\"Downloads the CIK lookup file and extracts all CIK numbers.\"\"\"\n",
        "    try:\n",
        "        print(\"Fetching CIK lookup data...\")\n",
        "        async with session.get(CIK_LOOKUP_URL, headers=LOOKUP_HEADERS) as resp:\n",
        "            await asyncio.sleep(DELAY_LOOKUP) # Be respectful to SEC servers\n",
        "            if resp.status != 200:\n",
        "                print(f\"Failed to fetch CIK lookup: Status {resp.status}\")\n",
        "                return []\n",
        "            content_type = resp.headers.get(\"Content-Type\", \"\")\n",
        "            if \"application/json\" not in content_type:\n",
        "                print(f\"Unexpected content-type for CIK lookup: {content_type}\")\n",
        "                return []\n",
        "            data = await resp.json()\n",
        "            ciks = [str(entry[\"cik_str\"]).zfill(10) for entry in data.values()]\n",
        "            print(f\"Successfully fetched {len(ciks)} CIKs.\")\n",
        "            return ciks\n",
        "    except Exception as e:\n",
        "        print(f\"Error during CIK lookup download: {e}\")\n",
        "        return []\n",
        "\n",
        "async def initialize_ciks():\n",
        "    \"\"\"Initializes the CIK list.\"\"\"\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        return await download_ciks(session)\n",
        "\n",
        "my_cik_list = [] # Will be populated by initialize_ciks\n",
        "CIK_SET     = set(my_cik_list)              # 4–10 k entities\n",
        "QUARTER_URL = \"https://www.sec.gov/Archives/edgar/full-index/2025/Q2/master.gz\" # Current date is May 16, 2025. Adjusted to Q2 2025.\n",
        "UA = \"MyResearchBot/1.0 (Muie; muie.restante@gmail.com) requests for academic research - contact for questions\"\n",
        "\n",
        "async def master_rows():\n",
        "    if not CIK_SET:\n",
        "        print(\"CIK_SET is empty. No data to fetch.\")\n",
        "        return\n",
        "    headers = {\n",
        "    \"User-Agent\": UA,\n",
        "    \"Accept\": \"*/*\",\n",
        "    \"Referer\": \"https://www.sec.gov/\",\n",
        "    \"Host\": \"www.sec.gov\"\n",
        "        }\n",
        "    async with aiohttp.ClientSession(headers=headers) as s:\n",
        "        async with s.get(QUARTER_URL, timeout=30) as r:\n",
        "            if r.status != 200:\n",
        "                print(f\"Failed to fetch master file: Status {r.status}\")\n",
        "                return\n",
        "            try:\n",
        "                content = await r.read()\n",
        "                decompressed_content = gzip.decompress(content).decode()\n",
        "                rows = decompressed_content.splitlines()\n",
        "                 # Find the header line index dynamically\n",
        "                header_line_index = -1\n",
        "                for i, line_content in enumerate(rows):\n",
        "                    if line_content.startswith(\"CIK|Company Name|Form Type|Date Filed|File Name\"):\n",
        "                        header_line_index = i\n",
        "                        break\n",
        "                \n",
        "                if header_line_index == -1:\n",
        "                    print(\"Master file header not found. Cannot parse.\")\n",
        "                    return\n",
        "\n",
        "                # Start processing from the line after the header\n",
        "                relevant_rows = rows[header_line_index + 1:]\n",
        "\n",
        "            except gzip.BadGzipFile:\n",
        "                print(f\"Error decompressing master file from {QUARTER_URL}. It might be corrupted or not gzipped.\")\n",
        "                # Attempt to read as plain text if decompression fails and it's small\n",
        "                if len(content) < 1024 * 1024: # 1MB threshold\n",
        "                    try:\n",
        "                        plain_text_content = content.decode()\n",
        "                        rows = plain_text_content.splitlines()\n",
        "                        header_line_index = -1\n",
        "                        for i, line_content in enumerate(rows):\n",
        "                            if line_content.startswith(\"CIK|Company Name|Form Type|Date Filed|File Name\"):\n",
        "                                header_line_index = i\n",
        "                                break\n",
        "                        if header_line_index == -1:\n",
        "                            print(\"Master file header not found in plain text attempt.\")\n",
        "                            return\n",
        "                        relevant_rows = rows[header_line_index + 1:]\n",
        "                        print(\"Successfully read master file as plain text after gzip error.\")\n",
        "                    except Exception as plain_text_err:\n",
        "                        print(f\"Could not read master file as plain text either: {plain_text_err}\")\n",
        "                        return\n",
        "                else:\n",
        "                    return # Don't process if large and ungzipped\n",
        "\n",
        "    for row_data in csv.reader(relevant_rows, delimiter=\"|\"):\n",
        "        if len(row_data) < 5: # Ensure row has enough columns\n",
        "            continue\n",
        "        cik, _, form, date, path = row_data\n",
        "        if form in FORM_TYPES and cik in CIK_SET:\n",
        "            yield cik.zfill(10), form, date, f\"https://www.sec.gov/Archives/{path}\"\n",
        "\n",
        "async def fetch_and_store(sema, row_info):\n",
        "    cik, form, date, url = row_info\n",
        "    # Ensure DATA_ROOT is defined and accessible\n",
        "    if not DATA_ROOT:\n",
        "        print(\"DATA_ROOT is not defined. Cannot save file.\")\n",
        "        return\n",
        "\n",
        "    dir_path = os.path.join(DATA_ROOT, cik, form)\n",
        "    fname = os.path.join(dir_path, f\"{date}.txt\")\n",
        "    \n",
        "    if os.path.exists(fname):                                 # incremental run\n",
        "        # print(f\"File {fname} already exists. Skipping.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Fetching: {url}\")\n",
        "    try:\n",
        "        async with sema, aiohttp.ClientSession(headers={\"User-Agent\": UA}) as s:\n",
        "            await asyncio.sleep(DELAY_LOOKUP) # Respect SEC rate limits\n",
        "            async with s.get(url, timeout=60) as r: # Increased timeout for potentially large files\n",
        "                if r.status == 200:\n",
        "                    txt = (await r.read()).decode(\"utf-8\", \"ignore\")\n",
        "                    os.makedirs(dir_path, exist_ok=True)\n",
        "                    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(txt)\n",
        "                    print(f\"Stored: {fname}\")\n",
        "                elif r.status == 404:\n",
        "                    print(f\"File not found (404): {url}\")\n",
        "                elif r.status == 403:\n",
        "                    print(f\"Access forbidden (403) for {url}. Check User-Agent or IP restrictions.\")\n",
        "                else:\n",
        "                    print(f\"Failed to fetch {url}: Status {r.status}\")\n",
        "    except asyncio.TimeoutError:\n",
        "        print(f\"Timeout fetching {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or storing {url}: {e}\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    global my_cik_list, CIK_SET\n",
        "    # Initialize CIKs first\n",
        "    fetched_ciks = await initialize_ciks()\n",
        "    if not fetched_ciks:\n",
        "        print(\"No CIKs were fetched. Exiting.\")\n",
        "        return\n",
        "    \n",
        "    my_cik_list = fetched_ciks\n",
        "    CIK_SET = set(my_cik_list)\n",
        "    print(f\"Working with {len(CIK_SET)} unique CIKs.\")\n",
        "\n",
        "    # Limit the number of CIKs for testing if needed\n",
        "    CIK_SET = set(list(CIK_SET)[:1]) # Example: use only the first 10 CIKs\n",
        "    print(f\"DEBUG: Using a subset of {len(CIK_SET)} CIKs for processing.\")\n",
        "\n",
        "\n",
        "    if not CIK_SET:\n",
        "        print(\"CIK list is empty after initialization. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    sema = asyncio.Semaphore(10) # Adjusted semaphore based on SEC's 10 req/s limit\n",
        "    \n",
        "    tasks = []\n",
        "    async for row_details in master_rows():\n",
        "        if row_details: # Ensure row_details is not None\n",
        "            tasks.append(fetch_and_store(sema, row_details))\n",
        "    \n",
        "    if tasks:\n",
        "        print(f\"Gathered {len(tasks)} tasks to process.\")\n",
        "        await asyncio.gather(*tasks)\n",
        "        print(\"All tasks completed.\")\n",
        "    else:\n",
        "        print(\"No tasks to process based on the master file and CIK list.\")\n",
        "\n",
        "if __name__ == '__main__': # Required for Colab/Jupyter if running asyncio directly\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 84/84 [06:56<00:00,  4.96s/files]\n",
            "Generating train split: 100%|██████████| 176289/176289 [03:04<00:00, 953.82 examples/s] \n",
            "Generating validation split: 100%|██████████| 22050/22050 [00:21<00:00, 1010.80 examples/s]\n",
            "Generating test split: 100%|██████████| 22036/22036 [00:22<00:00, 968.62 examples/s] \n",
            "Creating CSV from Arrow format: 100%|██████████| 177/177 [09:01<00:00,  3.06s/ba]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets cache directory is: C:\\Users\\Serban\\.cache\\huggingface\\datasets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Set the cache directory to a folder on your D: drive\n",
        "# Make sure this path exists or can be created.\n",
        "os.environ['HF_DATASETS_CACHE'] = 'D:\\\\huggingface_cache\\\\datasets' # Example path on D:\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "# Load the dataset for the 'train' split\n",
        "# The original command includes DatasetDict in its import, so we keep it.\n",
        "edgar_dataset_train = load_dataset('eloukas/edgar-corpus', split='train', trust_remote_code=True)\n",
        "# Save it to a CSV file\n",
        "edgar_dataset_train.to_csv('edgar_corpus_10k.csv')\n",
        "\n",
        "# You can optionally print the cache directory to confirm\n",
        "from datasets.config import HF_DATASETS_CACHE\n",
        "print(f\"Datasets cache directory is: {HF_DATASETS_CACHE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "chunksize = 10  # rows per chunk, tune based on your system\n",
        "for chunk in pd.read_csv(\"edgar_corpus_10k.csv\", chunksize=chunksize):\n",
        "    # Process each chunk\n",
        "    print(chunk.head())  # Example: preview first few rows of each chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['filename', 'cik', 'year', 'section_1', 'section_1A', 'section_1B',\n",
            "       'section_2', 'section_3', 'section_4', 'section_5', 'section_6',\n",
            "       'section_7', 'section_7A', 'section_8', 'section_9', 'section_9A',\n",
            "       'section_9B', 'section_10', 'section_11', 'section_12', 'section_13',\n",
            "       'section_14', 'section_15'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Find column names without loading the whole file\n",
        "df_sample = pd.read_csv(\"edgar_corpus_10k.csv\", nrows=5)\n",
        "print(df_sample.columns)  # See what you're working with\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 5 filings to sample_filings.txt\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define output file and sections of interest\n",
        "output_file = \"sample_filings.txt\"\n",
        "usecols = ['filename', 'cik', 'year', 'section_1', 'section_1A', 'section_1B',\n",
        "       'section_2', 'section_3', 'section_4', 'section_5', 'section_6',\n",
        "       'section_7', 'section_7A', 'section_8', 'section_9', 'section_9A',\n",
        "       'section_9B', 'section_10', 'section_11', 'section_12', 'section_13',\n",
        "       'section_14', 'section_15']\n",
        "chunksize = 1\n",
        "num_samples = 5  # how many filings you want to store\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
        "    for i, chunk in enumerate(pd.read_csv(\"edgar_corpus_10k.csv\", usecols=usecols, chunksize=chunksize)):\n",
        "        row = chunk.iloc[0]\n",
        "        out.write(f\"\\n\\n=== Filing {i + 1} ===\\n\")\n",
        "        out.write(f\"Filename: {row['filename']}\\n\")\n",
        "        out.write(f\"CIK: {row['cik']}\\n\")\n",
        "        out.write(f\"Year: {row['year']}\\n\")\n",
        "\n",
        "        for sec in ['section_1', 'section_1A', 'section_1B',\n",
        "       'section_2', 'section_3', 'section_4', 'section_5', 'section_6',\n",
        "       'section_7', 'section_7A', 'section_8', 'section_9', 'section_9A',\n",
        "       'section_9B', 'section_10', 'section_11', 'section_12', 'section_13',\n",
        "       'section_14', 'section_15']:\n",
        "            out.write(f\"\\n--- {sec} ---\\n\")\n",
        "            section_text = str(row[sec])\n",
        "            # You can write full text or limit it:\n",
        "            out.write(section_text)  # adjust char limit as needed\n",
        "        out.write(\"\\n\" + \"=\" * 20 + \"\\n\")\n",
        "\n",
        "        if i + 1 == num_samples:\n",
        "            break\n",
        "\n",
        "print(f\"Saved {num_samples} filings to {output_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
